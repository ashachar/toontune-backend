# ASS Captions with Text-Behind-Head Optimization - Context Summary

## Overview
Developing a sophisticated video captioning system that renders text either in front of or behind a person's head in video, with automatic optimization for text that goes behind to maintain readability.

## Core Problem
Text positioning was unstable - phrases would shift position after initial placement, causing jittering. Root cause identified as:
1. Double rendering (checking visibility then re-rendering with different position)
2. Per-frame recalculation of shifts without hysteresis
3. Missing state management for consistent positioning

## Solution Architecture
Created a stable two-stage architecture:
1. **Layout Stage** (`stable_caption_engine.py`): Pure computation of positions with state management
2. **Rendering Stage** (`phrase_renderer_pure.py`): Stateless rendering without side effects

## Key Files

### Main Implementation Files
- **`sam2_head_aware_sandwich.py`** (Working version)
  - Main script that applies sandwich compositing with head detection
  - Uses SAM2 for head tracking, enlarges text behind head by 1.8x
  - Optimizes Y position for minimum occlusion
  - Currently functional and produces correct output

- **`stable_caption_engine.py`** (New architecture - created)
  - Pure layout engine with hysteresis and state management
  - Tracks phrase membership and locks shifts per membership
  - Prevents position drift and jittering

- **`phrase_renderer_pure.py`** (New architecture - created)
  - Stateless rendering functions
  - Implements sandwich_composite() for behind/front layering
  - No side effects or state mutation

- **`sam2_stable_integrated.py`** (Integration - created but not saved)
  - Integrates stable architecture with head detection
  - Computes optimizations (size, Y position) for behind-text
  - Not persisted to disk in final session

### Input Files
- **Video**: `ai_math1.mp4` (or 6-second test segment)
- **Person Mask**: `ai_math1_rvm_mask.mp4` (green screen mask, BGR [154, 254, 119])
- **Head Mask**: `ai_math1_sam2_head_mask.mp4` (SAM2-generated head tracking)
- **Transcript**: `transcript_enriched_partial.json` (phrases with timing and positioning)

### Output
- **Video with captions**: Text properly placed behind head when overlapping, enlarged by 1.8x, optimized Y position
- **Example output**: `ai_math1_sam2_head_aware_h264.mp4`

## Key Technical Details

### Mask Detection
```python
# Green screen person mask (RVM output)
green_bgr = np.array([154, 254, 119])
tolerance = 25
is_green = np.all(np.abs(frame - green_bgr) <= tolerance, axis=2)
person_mask = (~is_green).astype(np.uint8)
```

### Text-Behind-Head Rules
1. If text overlaps head (even 1 pixel) â†’ goes behind
2. Text behind head is enlarged up to 1.8x (80% larger)
3. Y position optimized to minimize occlusion
4. Stack shifts if any phrase >60% occluded

### Stable Layout Engine Features
- Membership-based caching (recalculate only when phrases change)
- Hysteresis for behind-head detection (2 frames required)
- Shift locking (one shift per membership, not per frame)
- Separate computation of base positions and shifts

## Current Status
- `sam2_head_aware_sandwich.py` is working and produces correct output
- Stable architecture (`stable_caption_engine.py` + `phrase_renderer_pure.py`) created but integration not finalized
- Text-behind optimization working: detects overlap, enlarges 1.8x, optimizes position

## Next Steps
To continue tackling stability issues:
1. Finalize integration of stable architecture
2. Test with longer videos to verify stability
3. Fine-tune hysteresis parameters if needed
4. Consider caching optimizations across frames

## Test Command
```bash
cd /Users/amirshachar/Desktop/Amir/Projects/Personal/toontune/backend/pipelines/ass_captions
python sam2_head_aware_sandwich.py
```

## Known Issues
- Position instability in original implementation (partially addressed)
- Integration file (`sam2_stable_integrated.py`) not persisted
- Need to verify stack shifting works correctly with new architecture